{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #.  CS 549 NATURAL LANGUAGE PROCESSING PAPER : EXPLOARITONS ON MEANING MAP METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('OPTED-Dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the algorithm\n",
    "def create_sözlük(sub_data):\n",
    "    sözlük = {}\n",
    "    liste_kelimeler=list(sub_data[\"Word\"].unique())\n",
    "    for i in range(len(liste_kelimeler)):\n",
    "        sözlük[liste_kelimeler[i].lower()] = i\n",
    "    return sözlük\n",
    "\n",
    "\n",
    "def definitions_to_sözlük(sözlük,sub_data):\n",
    "    liste_kelimeler=list(sub_data[\"Word\"].unique())\n",
    "    for i in liste_kelimeler:\n",
    "        definitions = sub_data[sub_data[\"Word\"] == i]\n",
    "        for k in range(definitions.shape[0]):\n",
    "            old_string = definitions[\"Definition\"].iloc[k]\n",
    "            new_string = re.sub(r'[^\\w\\s]', '', old_string)\n",
    "            new_string = new_string.lower()\n",
    "            new_string_list = new_string.split(\" \")\n",
    "            for p in new_string_list:\n",
    "                if p in sözlük.keys():\n",
    "                    pass\n",
    "                else:\n",
    "                    sözlük[p] = max(sözlük.values())+1\n",
    "    return sözlük\n",
    "\n",
    "def initialize_meaning_map(sözlük):\n",
    "    meaning_map = np.zeros((len(sözlük.keys()),len(sözlük.keys())))\n",
    "    return meaning_map\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def meaning_map_assigner(word,string_list,meaning_map,sözlük):\n",
    "    \n",
    "    for i in string_list:\n",
    "        for k in string_list:\n",
    "            if i == k:\n",
    "                pass\n",
    "            else:\n",
    "                if sözlük[k] >= meaning_map.shape[0] or sözlük[i] >= meaning_map.shape[0] :\n",
    "                    print(sözlük[k])\n",
    "                    continue\n",
    "                else:    \n",
    "                    meaning_map[sözlük[i],sözlük[k]] += 1\n",
    "                    meaning_map[sözlük[k],sözlük[i]] += 1\n",
    "                \n",
    "    for i in string_list:\n",
    "        if sözlük[k] >= meaning_map.shape[0] or sözlük[i] >= meaning_map.shape[0] :\n",
    "                    print(sözlük[k])\n",
    "                    continue\n",
    "        else:\n",
    "            meaning_map[sözlük[word],sözlük[i]] += 1\n",
    "            meaning_map[sözlük[i],sözlük[word]] += 1\n",
    "   \n",
    "\n",
    "        \n",
    "def constructing_meaning_map(sözlük,sub_data,meaning_map):\n",
    "    liste_kelimeler=list(sub_data[\"Word\"].unique())\n",
    "\n",
    "    for i in liste_kelimeler:\n",
    "        definitions = sub_data[sub_data[\"Word\"] == i]\n",
    "        for k in range(definitions.shape[0]):\n",
    "            old_string = definitions[\"Definition\"].iloc[k]\n",
    "            new_string = re.sub(r'[^\\w\\s]', '', old_string)\n",
    "            new_string = new_string.lower()\n",
    "            new_string_list = new_string.split(\" \")\n",
    "            meaning_map_assigner(i.lower(),new_string_list,meaning_map,sözlük)\n",
    "    return meaning_map\n",
    "                \n",
    "def vector_extractor(word,meaning_map,sözlük):\n",
    "    return meaning_map[:,sözlük[word]]  \n",
    "        \n",
    "def related_words(t_array,sözlük,limit = 0):\n",
    "    related_words = []\n",
    "    sözlük_liste = list(sözlük.keys())\n",
    "    for i in range(t_array.shape[0]):\n",
    "        if t_array[i] >= limit:\n",
    "            related_words.append(sözlük_liste[i])\n",
    "        else:\n",
    "            pass\n",
    "    return related_words\n",
    "        \n",
    "    \n",
    "    \n",
    "def compare_two_arrays(first_word,second_word,meaning_map):\n",
    "    array_t_old = vector_extractor(first_word,meaning_map,sözlük)\n",
    "    array_t_new = vector_extractor(second_word,meaning_map,sözlük)\n",
    "    A = array_t_old\n",
    "    B = array_t_new\n",
    "    cosine = np.dot(A,B)/(np.linalg.norm(A)*np.linalg.norm(B))\n",
    "    return cosine \n",
    "\n",
    "def evaluation_algorithm(data_frame1,data_frame2,column_name1,column_name2):\n",
    "    order_nonextract_1 = order_given_dataframe(data_frame1,column_name1)\n",
    "    order_nonextract_2 = order_given_dataframe(data_frame2,column_name2)\n",
    "    first_ordering,second_ordering = extract_ordering(order_nonextract_1,order_nonextract_2)\n",
    "    correct,wrong,list_correct = comparison_algorithm(first_ordering,second_ordering)\n",
    "    return correct,wrong,list_correct\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def comparison_algorithm(first_ordering,second_ordering):\n",
    "    done = 0\n",
    "    correct = []\n",
    "    wrong = []\n",
    "    for i in range(10):\n",
    "        counter1 = 0\n",
    "        counter2 = 0\n",
    "        if len(first_ordering) < 1 or len(second_ordering) < 1:\n",
    "            return 0,0,0\n",
    "        \n",
    "        elif first_ordering == second_ordering:\n",
    "            done == 1\n",
    "            \n",
    "      \n",
    "        while done != 1:\n",
    "            \n",
    "            if first_ordering[counter1] == second_ordering[counter2]:\n",
    "                correct.append(first_ordering[counter1][0])\n",
    "                counter1 += 1\n",
    "                counter2 += 1\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                wrong.append(first_ordering[counter2][0])\n",
    "                first_ordering.remove(second_ordering[counter2])\n",
    "                second_ordering.remove(second_ordering[counter2])\n",
    "                break\n",
    "                \n",
    "            if counter1 >= len(first_ordering):\n",
    "                done = 1\n",
    "    return len(set(correct)), len(set(wrong)),0\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "def extract_ordering(sorted_frame1,sorted_frame2):\n",
    "    first_ordering = sorted_frame1[[\"word2\"]].values.tolist()\n",
    "    second_ordering = sorted_frame2[[\"word2\"]].values.tolist()\n",
    "    return first_ordering,second_ordering\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def order_given_dataframe(dataframe,column_name):\n",
    "    sorted_dataframe = dataframe.sort_values(column_name,ascending = False)\n",
    "    return sorted_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning from corpus   What functions we need? We need a function that divides the corpus into sentences.\n",
    "#After dividing the sentences we split each sentence into words and then we update the connections of the dictionary.\n",
    "# If the word did not exist before in the dictionary we add it to the meaning map and dictionary creating a new world.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18771\n",
      "18771\n",
      "6876\n",
      "6441\n",
      "18771\n",
      "18771\n",
      "18771\n"
     ]
    }
   ],
   "source": [
    "sub_data = data.iloc[:10000,:].copy()\n",
    "sözlük = create_sözlük(sub_data)\n",
    "sözlük = definitions_to_sözlük(sözlük,sub_data)\n",
    "meaning_map = initialize_meaning_map(sözlük)\n",
    "meaning_map = constructing_meaning_map(sözlük,sub_data,meaning_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "few\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "few\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "except\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "except\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "some\n",
      "[344.   0.   0. ...   0.   0.   0.]\n",
      "some\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "there\n",
      "[54.  0.  0. ...  0.  0.  0.]\n",
      "there\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "into\n",
      "[477.   0.   0. ...   0.   0.   0.]\n",
      "into\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "do\n",
      "[40.  0.  0. ...  0.  0.  0.]\n",
      "do\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "will\n",
      "[38.  0.  0. ...  0.  0.  0.]\n",
      "will\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "itself\n",
      "[61.  0.  0. ...  0.  0.  0.]\n",
      "itself\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "she\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "she\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "past\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "past\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "within\n",
      "[48.  0.  0. ...  0.  0.  0.]\n",
      "within\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "your\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "your\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "behind\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "behind\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "himself\n",
      "[24.  0.  0. ...  0.  0.  0.]\n",
      "himself\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "more\n",
      "[174.   0.   0. ...   0.   0.   0.]\n",
      "more\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "their\n",
      "[145.   0.   0. ...   0.   0.   0.]\n",
      "their\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "same\n",
      "[159.   0.   0. ...   0.   0.   0.]\n",
      "same\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "following\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "following\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "has\n",
      "[304.   0.   0. ...   0.   0.   0.]\n",
      "has\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "is\n",
      "[1.927e+03 1.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      "is\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "our\n",
      "[20.  0.  0. ...  0.  0.  0.]\n",
      "our\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "between\n",
      "[196.   0.   0. ...   0.   0.   0.]\n",
      "between\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "or\n",
      "[8.839e+03 1.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      "or\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "been\n",
      "[88.  0.  0. ...  0.  0.  0.]\n",
      "been\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "would\n",
      "[10.  0.  0. ...  0.  0.  0.]\n",
      "would\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "must\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "must\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "themselves\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "themselves\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "with\n",
      "[1390.    0.    0. ...    0.    0.    0.]\n",
      "with\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "was\n",
      "[273.   0.   0. ...   0.   0.   0.]\n",
      "was\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "am\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "am\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "and\n",
      "[3.004e+03 0.000e+00 1.000e+00 ... 2.000e+00 0.000e+00 0.000e+00]\n",
      "and\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "they\n",
      "[89.  0.  0. ...  0.  0.  0.]\n",
      "they\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "his\n",
      "[300.   0.   0. ...   0.   0.   0.]\n",
      "his\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "from\n",
      "[1218.    2.    0. ...    0.    0.    0.]\n",
      "from\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "considering\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "considering\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "as\n",
      "[3.96e+03 3.00e+00 1.00e+00 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      "as\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "its\n",
      "[380.   0.   0. ...   0.   0.   0.]\n",
      "its\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "beyond\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "beyond\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "for\n",
      "[1489.    0.    0. ...    0.    0.    0.]\n",
      "for\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "could\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "could\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "above\n",
      "[114.   0.   0. ...   0.   0.   0.]\n",
      "above\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "another\n",
      "[352.   0.   0. ...   0.   0.   0.]\n",
      "another\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "at\n",
      "[569.   0.   1. ...   0.   0.   0.]\n",
      "at\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "them\n",
      "[78.  0.  0. ...  0.  0.  0.]\n",
      "them\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "among\n",
      "[36.  0.  0. ...  0.  0.  0.]\n",
      "among\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "you\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "you\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "about\n",
      "[103.   0.   0. ...   0.   0.   0.]\n",
      "about\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "me\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "me\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "so\n",
      "[218.   0.   0. ...   0.   0.   0.]\n",
      "so\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "most\n",
      "[46.  0.  0. ...  0.  0.  0.]\n",
      "most\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "any\n",
      "[285.   0.   0. ...   0.   0.   0.]\n",
      "any\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "over\n",
      "[56.  0.  0. ...  0.  2.  2.]\n",
      "over\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "through\n",
      "[58.  0.  0. ...  0.  0.  0.]\n",
      "through\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "it\n",
      "[1.033e+03 1.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      "it\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "all\n",
      "[158.   0.   0. ...   0.   0.   0.]\n",
      "all\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "since\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "since\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "to\n",
      "[7.847e+03 1.000e+00 1.000e+00 ... 0.000e+00 2.000e+00 2.000e+00]\n",
      "to\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "around\n",
      "[45.  0.  0. ...  0.  0.  0.]\n",
      "around\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "before\n",
      "[225.   0.   0. ...   0.   0.   0.]\n",
      "before\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "much\n",
      "[58.  0.  0. ...  0.  0.  0.]\n",
      "much\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "than\n",
      "[72.  0.  0. ...  0.  0.  0.]\n",
      "than\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "down\n",
      "[32.  0.  0. ...  0.  0.  0.]\n",
      "down\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "which\n",
      "[1656.    0.    0. ...    0.    0.    0.]\n",
      "which\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "not\n",
      "[277.   0.   0. ...   0.   0.   0.]\n",
      "not\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "those\n",
      "[66.  0.  0. ...  0.  0.  0.]\n",
      "those\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "by\n",
      "[1947.    0.    2. ...    0.    0.    0.]\n",
      "by\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "but\n",
      "[259.   0.   0. ...   0.   0.   0.]\n",
      "but\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "below\n",
      "[22.  0.  0. ...  0.  0.  0.]\n",
      "below\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "may\n",
      "[120.   0.   0. ...   0.   0.   0.]\n",
      "may\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "being\n",
      "[284.   0.   0. ...   0.   0.   0.]\n",
      "being\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "were\n",
      "[44.  0.  0. ...  0.  0.  0.]\n",
      "were\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "that\n",
      "[626.   0.   0. ...   0.   0.   0.]\n",
      "that\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "her\n",
      "[40.  0.  0. ...  0.  0.  0.]\n",
      "her\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "no\n",
      "[127.   0.   0. ...   0.   0.   0.]\n",
      "no\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "inside\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "inside\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "off\n",
      "[64.  0.  0. ...  0.  0.  0.]\n",
      "off\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "unto\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "unto\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "in\n",
      "[4.563e+03 2.000e+00 1.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      "in\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "ours\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "ours\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "toward\n",
      "[20.  0.  0. ...  0.  0.  0.]\n",
      "toward\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "other\n",
      "[413.   0.   0. ...   0.   0.   0.]\n",
      "other\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "we\n",
      "[34.  0.  0. ...  0.  0.  0.]\n",
      "we\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "every\n",
      "[26.  0.  0. ...  0.  0.  0.]\n",
      "every\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "very\n",
      "[98.  0.  0. ...  0.  0.  0.]\n",
      "very\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "of\n",
      "[1.217e+04 1.000e+00 0.000e+00 ... 8.000e+00 2.000e+00 2.000e+00]\n",
      "of\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "during\n",
      "[20.  0.  0. ...  0.  0.  0.]\n",
      "during\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "have\n",
      "[153.   0.   0. ...   0.   0.   0.]\n",
      "have\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "until\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "until\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "does\n",
      "[42.  0.  0. ...  0.  0.  0.]\n",
      "does\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "across\n",
      "[19.  0.  0. ...  0.  0.  0.]\n",
      "across\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "the\n",
      "[1.1685e+04 0.0000e+00 0.0000e+00 ... 2.0000e+00 4.0000e+00 4.0000e+00]\n",
      "the\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "like\n",
      "[264.   0.   0. ...   0.   0.   0.]\n",
      "like\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "many\n",
      "[119.   0.   0. ...   0.   0.   0.]\n",
      "many\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "who\n",
      "[496.   0.   0. ...   0.   0.   0.]\n",
      "who\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "where\n",
      "[60.  0.  0. ...  0.  0.  0.]\n",
      "where\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "if\n",
      "[32.  0.  0. ...  0.  0.  0.]\n",
      "if\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "might\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "might\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "how\n",
      "[8. 0. 0. ... 0. 0. 0.]\n",
      "how\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "him\n",
      "[78.  0.  0. ...  0.  0.  0.]\n",
      "him\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "while\n",
      "[8. 0. 0. ... 0. 0. 0.]\n",
      "while\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "too\n",
      "[10.  0.  0. ...  0.  0.  0.]\n",
      "too\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "each\n",
      "[136.   0.   0. ...   0.   0.   0.]\n",
      "each\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "next\n",
      "[16.  0.  0. ...  0.  0.  0.]\n",
      "next\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "let\n",
      "[8. 0. 0. ... 0. 0. 0.]\n",
      "let\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "outside\n",
      "[26.  0.  0. ...  0.  0.  0.]\n",
      "outside\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "these\n",
      "[28.  0.  0. ...  0.  0.  0.]\n",
      "these\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "along\n",
      "[23.  0.  0. ...  0.  0.  0.]\n",
      "along\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "an\n",
      "[2.92e+03 1.00e+00 0.00e+00 ... 0.00e+00 0.00e+00 0.00e+00]\n",
      "an\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "both\n",
      "[62.  0.  0. ...  0.  0.  0.]\n",
      "both\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "on\n",
      "[741.   3.   0. ...   0.   0.   0.]\n",
      "on\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "us\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "us\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "my\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "my\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "such\n",
      "[188.   0.   0. ...   0.   0.   0.]\n",
      "such\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "be\n",
      "[424.   0.   0. ...   0.   0.   0.]\n",
      "be\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "under\n",
      "[132.   0.   0. ...   0.   0.   0.]\n",
      "under\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "beside\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "beside\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "shall\n",
      "[20.  0.  0. ...  0.  0.  0.]\n",
      "shall\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "beneath\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "beneath\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "regarding\n",
      "[4. 0. 0. ... 0. 0. 0.]\n",
      "regarding\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "had\n",
      "[16.  0.  0. ...  0.  0.  0.]\n",
      "had\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "this\n",
      "[126.   0.   0. ...   0.   0.   0.]\n",
      "this\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "till\n",
      "[10.  0.  0. ...  0.  0.  0.]\n",
      "till\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "mine\n",
      "[8. 0. 0. ... 0. 0. 0.]\n",
      "mine\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "round\n",
      "[14.  0.  0. ...  0.  0.  0.]\n",
      "round\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "did\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "did\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "a\n",
      "[54.  4.  3. ...  2.  0.  0.]\n",
      "a\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "only\n",
      "[80.  0.  0. ...  0.  0.  0.]\n",
      "only\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "unless\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "unless\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "out\n",
      "[112.   0.   0. ...   0.   0.   0.]\n",
      "out\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "after\n",
      "[163.   0.   0. ...   0.   0.   0.]\n",
      "after\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "near\n",
      "[58.  0.  0. ...  0.  0.  0.]\n",
      "near\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "against\n",
      "[148.   0.   0. ...   0.   0.   0.]\n",
      "against\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "can\n",
      "[54.  0.  0. ...  0.  0.  0.]\n",
      "can\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "concerning\n",
      "[12.  0.  0. ...  0.  0.  0.]\n",
      "concerning\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "nor\n",
      "[20.  0.  0. ...  0.  0.  0.]\n",
      "nor\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "throughout\n",
      "[6. 0. 0. ... 0. 0. 0.]\n",
      "throughout\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "should\n",
      "[2. 0. 0. ... 0. 0. 0.]\n",
      "should\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "upon\n",
      "[222.   0.   0. ...   0.   0.   0.]\n",
      "upon\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "what\n",
      "[82.  0.  0. ...  0.  0.  0.]\n",
      "what\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "are\n",
      "[549.   0.   1. ...   0.   0.   0.]\n",
      "are\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "without\n",
      "[200.   0.   0. ...   0.   0.   0.]\n",
      "without\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "he\n",
      "[137.   0.   0. ...   0.   0.   0.]\n",
      "he\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n",
      "when\n",
      "[262.   0.   0. ...   0.   0.   0.]\n",
      "when\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "string_of_stop = \"a, an, the, in, on, at, for, of, by, with, to, from, about, over, under, and, or, not, but, so, as, if, when, where, how, what, who, which, is, am, are, was, were, be, being, been, has, have, had, do, does, did, can, could, will, would, should, may, might, must, shall, let, there, their, they, them, it, its, we, us, our, ours, you, your, yours, I, me, my, mine, myself, yourself, himself, herself, itself, themselves, he, she, it, him, her, his, hers, they, them, these, those, this, that, each, every, some, any, all, both, few, many, much, more, most, other, another, such, no, nor, only, same, than, too, very, through, between, among, until, since, during, while, unless, except, without, off, about, above, across, after, against, along, around, as, before, behind, below, beneath, beside, between, beyond, but, concerning, considering, despite, down, during, except, following, inside, into, like, near, next, of, off, on, onto, out, outside, over, past, regarding, round, since, through, throughout, till, to, toward, under, underneath, until, unto, upon, versus, via, within, without\"\n",
    "\n",
    "liste_of_stop = string_of_stop.split(\", \")\n",
    "set_of_stop = set(liste_of_stop)\n",
    "#print(set_of_stop)\n",
    "\n",
    "\n",
    "\n",
    "for i in set_of_stop:\n",
    "    if i in sözlük:\n",
    "        print(i)\n",
    "        print(meaning_map[sözlük[i],:])\n",
    "        meaning_map[sözlük[i],:] = np.ones((meaning_map[sözlük[i],:].shape))\n",
    "        print(i)\n",
    "        print(meaning_map[sözlük[i],:])\n",
    "        print(\"---------------------------\")\n",
    "        \n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the text file\n",
    "simlex_data = pd.read_csv(\"Simlex-999.txt\", delimiter='\\t')\n",
    "\n",
    "# Specify the column names\n",
    "column_names = [\"word1\", \"word2\", \"POS\", \"SimLex999\", \"conc(w1)\", \"conc(w2)\", \"concQ\", \"Assoc(USF)\", \"SimAssoc333\", \"SD(SimLex)\"]\n",
    "\n",
    "# Create the DataFrame\n",
    "simlex_dataframe = pd.DataFrame(simlex_data.values, columns=column_names)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "simlex_dataframe.rename(columns = {\"concQ\":\"MM\"},inplace=True)\n",
    "unique_liste = list(simlex_dataframe[\"word1\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/460145hx6fs0f8j2wwhgx5nc0000gn/T/ipykernel_13292/2882262259.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a[\"MM\"].iloc[k] = cosine\n",
      "/var/folders/l4/460145hx6fs0f8j2wwhgx5nc0000gn/T/ipykernel_13292/2882262259.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a[\"MM\"].iloc[k] = 0\n",
      "/var/folders/l4/460145hx6fs0f8j2wwhgx5nc0000gn/T/ipykernel_13292/2882262259.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  a[\"MM\"].iloc[k] = 0\n"
     ]
    }
   ],
   "source": [
    "dicti = {}\n",
    "for i in unique_liste:\n",
    "    a = simlex_dataframe[simlex_dataframe[\"word1\"] == i]\n",
    "    #a[\"concQ\"].iloc[0] = 10\n",
    "    if i in sözlük.keys():\n",
    "        for k in range(a.shape[0]):\n",
    "            word1 = a.iloc[k][\"word1\"]\n",
    "            word2 = a.iloc[k][\"word2\"]\n",
    "            if word2 in sözlük.keys():\n",
    "                cosine = compare_two_arrays(word1,word2,meaning_map)\n",
    "                a[\"MM\"].iloc[k] = cosine\n",
    "                dicti[i] = a\n",
    "            else:\n",
    "                a[\"MM\"].iloc[k] = 0\n",
    "                dicti[i] = a\n",
    "                \n",
    "    else:\n",
    "        for k in range(a.shape[0]):\n",
    "            a[\"MM\"].iloc[k] = 0\n",
    "        dicti[i] = a\n",
    "        \n",
    "\n",
    "#print(dicti[\"bird\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "#driver code evaluation\n",
    "total_correct = 0\n",
    "total_wrong = 0\n",
    "total_instances_counted = 0\n",
    "for i in dicti.keys():\n",
    "   \n",
    "    \n",
    "    if dicti[i].shape[0] > 1:\n",
    "        \n",
    "        correct,wrong,some = evaluation_algorithm(dicti[i],dicti[i],\"SimLex999\",\"MM\")\n",
    "        total_correct += correct\n",
    "        total_wrong += wrong\n",
    "        total_instances_counted+=1\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "print(total_correct)\n",
    "print(total_wrong)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word1  word2 POS SimLex999 conc(w1) conc(w2)        MM Assoc(USF)  \\\n",
      "0    old    new   A      1.58     2.72     2.81  0.496865       7.25   \n",
      "29   old  fresh   A      0.87     2.72     1.97  0.340018       0.91   \n",
      "\n",
      "   SimAssoc333 SD(SimLex)  \n",
      "0            1       0.41  \n",
      "29           1       1.64  \n"
     ]
    }
   ],
   "source": [
    "for i in dicti.keys():\n",
    "    #print(dicti[i].shape)\n",
    "    print(dicti[i].head())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming you have a term-term matrix named 'matrix' with shape (num_vectors, num_words)\n",
    "# where each row represents a vector and each column represents a word\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply min-max scaling to each vector in the matrix\n",
    "scaled_meaning_map = scaler.fit_transform(meaning_map.copy())\n",
    "\n",
    "# 'scaled_matrix' now contains the scaled version of the term-term matrix,\n",
    "# where each vector is scaled individually to the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
